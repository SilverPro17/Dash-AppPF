{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2cae5b",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados (EDA) - Dataset Principal de Plâncton (CPR)\n",
    "\n",
    "Este notebook realiza a análise exploratória inicial do dataset Continuous Plankton Recorder (CPR) descarregado do GBIF.\n",
    "\n",
    "**Formato:** Darwin Core Archive (DwC-A)\n",
    "**Ficheiro Principal:** `occurrence.txt` (~3.2GB)\n",
    "\n",
    "**Nota:** Devido ao tamanho do ficheiro `occurrence.txt`, a leitura e análise serão feitas usando `chunking` com Pandas para gerir o uso de memória.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# Definir diretórios\n",
    "project_dir = \"/home/ubuntu/plankton_project\"\n",
    "data_dir = os.path.join(project_dir, \"data\")\n",
    "plankton_data_dir = os.path.join(data_dir, \"plankton_dwca\")\n",
    "report_dir = os.path.join(project_dir, \"report\")\n",
    "\n",
    "# Criar diretório de report se não existir\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "\n",
    "# Caminho para o ficheiro principal\n",
    "occurrence_file = os.path.join(plankton_data_dir, \"occurrence.txt\")\n",
    "\n",
    "print(f\"Ficheiro de ocorrências: {occurrence_file}\")\n",
    "print(\"Bibliotecas importadas com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd759dd",
   "metadata": {},
   "source": [
    "## 1. Leitura Inicial e Inspeção (Primeiro Chunk)\n",
    "\n",
    "Vamos ler apenas o primeiro chunk do ficheiro `occurrence.txt` para inspecionar as colunas, tipos de dados e ter uma ideia inicial do conteúdo sem carregar o ficheiro inteiro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o tamanho do chunk (número de linhas)\n",
    "chunk_size = 100000 # Ajustar conforme necessário\n",
    "first_chunk = None # Inicializar a variável\n",
    "\n",
    "try:\n",
    "    print(f\"A ler o primeiro chunk ({chunk_size} linhas) de {occurrence_file}...\")\n",
    "    # Ler o primeiro chunk, assumindo separador TAB (comum em DwC-A)\n",
    "    # Adicionar error_bad_lines=False ou on_bad_lines=\"skip\" se houver linhas mal formatadas\n",
    "    # Adicionar low_memory=False se houver problemas com tipos mistos\n",
    "    chunk_iter = pd.read_csv(occurrence_file, sep=\"\\t\", chunksize=chunk_size, low_memory=False, on_bad_lines=\"warn\")\n",
    "    first_chunk = next(chunk_iter)\n",
    "    print(\"Primeiro chunk lido com sucesso.\")\n",
    "\n",
    "    # Inspeção inicial\n",
    "    print(\"\\nInformações do Primeiro Chunk:\")\n",
    "    first_chunk.info()\n",
    "\n",
    "    print(\"\\nPrimeiras 5 linhas:\")\n",
    "    # Usar display para melhor formatação no Jupyter\n",
    "    from IPython.display import display\n",
    "    display(first_chunk.head())\n",
    "\n",
    "    print(\"\\nColunas disponíveis:\")\n",
    "    print(list(first_chunk.columns))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Ficheiro não encontrado em {occurrence_file}\")\n",
    "except StopIteration:\n",
    "    print(\"Erro: O ficheiro parece estar vazio ou o chunksize é maior que o ficheiro.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o primeiro chunk: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9d35f",
   "metadata": {},
   "source": [
    "## 2. Processamento Completo e Agregação (Todos os Chunks)\n",
    "\n",
    "Agora, vamos iterar sobre todos os chunks do ficheiro `occurrence.txt` para:\n",
    "- Selecionar colunas relevantes.\n",
    "- Converter tipos de dados (datas, coordenadas).\n",
    "- Realizar limpeza básica.\n",
    "- Agregar estatísticas gerais (contagem total, intervalo temporal, espécies, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b048ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas de interesse (ajustar conforme necessário)\n",
    "use_cols = [\n",
    "    'gbifID', 'eventDate', 'year', 'month', 'day',\n",
    "    'decimalLatitude', 'decimalLongitude',\n",
    "    'scientificName', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus',\n",
    "    'basisOfRecord', 'occurrenceStatus', 'individualCount'\n",
    "]\n",
    "\n",
    "# Tipos de dados esperados (para otimização e consistência)\n",
    "dtypes = {\n",
    "    'gbifID': 'int64',\n",
    "    'year': 'Int64', # Usar tipo Int64 que suporta NA\n",
    "    'month': 'Int64',\n",
    "    'day': 'Int64',\n",
    "    'decimalLatitude': 'float64',\n",
    "    'decimalLongitude': 'float64',\n",
    "    'scientificName': 'str',\n",
    "    'kingdom': 'str',\n",
    "    'phylum': 'str',\n",
    "    'class': 'str',\n",
    "    'order': 'str',\n",
    "    'family': 'str',\n",
    "    'genus': 'str',\n",
    "    'basisOfRecord': 'str',\n",
    "    'occurrenceStatus': 'str',\n",
    "    'individualCount': 'float64' # Pode ser float devido a NA ou contagens não inteiras\n",
    "}\n",
    "\n",
    "# Variáveis para agregação\n",
    "total_records = 0\n",
    "all_species = set()\n",
    "min_date, max_date = pd.Timestamp.max, pd.Timestamp.min\n",
    "min_lat, max_lat = 90.0, -90.0\n",
    "min_lon, max_lon = 180.0, -180.0\n",
    "species_counts = pd.Series(dtype='int64')\n",
    "records_per_year = pd.Series(dtype='int64')\n",
    "\n",
    "# Lista para guardar amostras de dados para visualização (se necessário)\n",
    "data_samples = []\n",
    "sample_fraction = 0.01 # Guardar 1% dos dados para visualização\n",
    "\n",
    "print(f\"Iniciando processamento completo de {occurrence_file} em chunks de {chunk_size} linhas...\")\n",
    "\n",
    "try:\n",
    "    chunk_iter = pd.read_csv(\n",
    "        occurrence_file,\n",
    "        sep='\\t',\n",
    "        chunksize=chunk_size,\n",
    "        usecols=lambda c: c in use_cols, # Ler apenas colunas de interesse\n",
    "        dtype={k: dtypes.get(k, 'object') for k in use_cols}, # Aplicar dtypes, object para os não especificados\n",
    "        low_memory=False,\n",
    "        on_bad_lines='warn' # Avisar sobre linhas problemáticas\n",
    "    )\n",
    "\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        print(f\"Processando chunk {i+1}...\")\n",
    "        total_records += len(chunk)\n",
    "\n",
    "        # Limpeza e Conversão de Tipos\n",
    "        # Converter coordenadas para numérico, coercing erros para NaN\n",
    "        chunk['decimalLatitude'] = pd.to_numeric(chunk['decimalLatitude'], errors='coerce')\n",
    "        chunk['decimalLongitude'] = pd.to_numeric(chunk['decimalLongitude'], errors='coerce')\n",
    "\n",
    "        # Tentar construir a data a partir de year, month, day\n",
    "        # Usar Int64 para permitir NA em colunas de inteiros\n",
    "        chunk[['year', 'month', 'day']] = chunk[['year', 'month', 'day']].astype('Int64')\n",
    "        # Construir data apenas se year, month, day forem válidos\n",
    "        valid_date_idx = chunk[['year', 'month', 'day']].notna().all(axis=1)\n",
    "        # Usar um formato explícito e errors='coerce' para datas inválidas\n",
    "        chunk.loc[valid_date_idx, 'parsedDate'] = pd.to_datetime(\n",
    "            chunk.loc[valid_date_idx, ['year', 'month', 'day']],\n",
    "            format='%Y%m%d', errors='coerce'\n",
    "        )\n",
    "        # Tentar preencher com eventDate se a construção falhar ou não for possível\n",
    "        chunk['parsedDate'] = chunk['parsedDate'].fillna(pd.to_datetime(chunk['eventDate'], errors='coerce'))\n",
    "\n",
    "        # Remover linhas sem data ou coordenadas válidas\n",
    "        chunk.dropna(subset=['parsedDate', 'decimalLatitude', 'decimalLongitude'], inplace=True)\n",
    "\n",
    "        if not chunk.empty:\n",
    "            # Atualizar agregados\n",
    "            min_date = min(min_date, chunk['parsedDate'].min())\n",
    "            max_date = max(max_date, chunk['parsedDate'].max())\n",
    "            min_lat = min(min_lat, chunk['decimalLatitude'].min())\n",
    "            max_lat = max(max_lat, chunk['decimalLatitude'].max())\n",
    "            min_lon = min(min_lon, chunk['decimalLongitude'].min())\n",
    "            max_lon = max(max_lon, chunk['decimalLongitude'].max())\n",
    "\n",
    "            all_species.update(chunk['scientificName'].dropna().unique())\n",
    "            species_counts = species_counts.add(chunk['scientificName'].value_counts(), fill_value=0)\n",
    "            records_per_year = records_per_year.add(chunk['parsedDate'].dt.year.value_counts(), fill_value=0)\n",
    "\n",
    "            # Guardar uma amostra\n",
    "            data_samples.append(chunk.sample(frac=sample_fraction))\n",
    "\n",
    "    print(\"Processamento de chunks concluído.\")\n",
    "\n",
    "    # Concatenar amostras\n",
    "    if data_samples:\n",
    "        sample_df = pd.concat(data_samples, ignore_index=True)\n",
    "        print(f\"DataFrame de amostra criado com {len(sample_df)} registos.\")\n",
    "    else:\n",
    "        sample_df = pd.DataFrame(columns=use_cols + ['parsedDate']) # Criar df vazio se não houver amostras\n",
    "        print(\"Nenhuma amostra de dados foi criada.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante o processamento dos chunks: {e}\")\n",
    "    # Pode ser útil imprimir o estado das variáveis de agregação aqui\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9675867",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Estatísticas Agregadas ---\")\n",
    "print(f\"Total de registos processados (após limpeza inicial): {total_records}\")\n",
    "if min_date != pd.Timestamp.max:\n",
    "    print(f\"Intervalo Temporal: {min_date.strftime('%Y-%m-%d')} a {max_date.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(\"Intervalo Temporal: Não foi possível determinar.\")\n",
    "print(f\"Cobertura Geográfica (Lat): {min_lat:.4f} a {max_lat:.4f}\")\n",
    "print(f\"Cobertura Geográfica (Lon): {min_lon:.4f} a {max_lon:.4f}\")\n",
    "print(f\"Número de espécies únicas identificadas: {len(all_species)}\")\n",
    "\n",
    "print(\"\\nTop 10 Espécies Mais Frequentes:\")\n",
    "print(species_counts.astype(int).nlargest(10))\n",
    "\n",
    "print(\"\\nNúmero de Registos por Ano (Top 10 anos):\")\n",
    "print(records_per_year.astype(int).nlargest(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289231a4",
   "metadata": {},
   "source": [
    "## 3. Visualizações Preliminares\n",
    "\n",
    "Vamos criar algumas visualizações com base nos dados agregados e na amostra recolhida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef8854",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'species_counts' in locals() and not species_counts.empty:\n",
    "    # Plot Top 10 Espécies\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_species = species_counts.astype(int).nlargest(10)\n",
    "    sns.barplot(x=top_species.values, y=top_species.index, palette='viridis')\n",
    "    plt.title('Top 10 Espécies Mais Frequentes')\n",
    "    plt.xlabel('Número de Ocorrências')\n",
    "    plt.ylabel('Nome Científico')\n",
    "    plt.tight_layout()\n",
    "    save_path_species = os.path.join(report_dir, 'top_10_species.png')\n",
    "    plt.savefig(save_path_species)\n",
    "    print(f\"Gráfico Top 10 Espécies salvo em: {save_path_species}\")\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Dados de contagem de espécies não disponíveis para plotagem.\")\n",
    "\n",
    "if 'records_per_year' in locals() and not records_per_year.empty:\n",
    "    # Plot Registos por Ano\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Ordenar por ano para o gráfico de linha\n",
    "    records_per_year_sorted = records_per_year.astype(int).sort_index()\n",
    "    records_per_year_sorted.plot(kind='line', marker='o') # Usar gráfico de linha\n",
    "    plt.title('Número Total de Registos por Ano')\n",
    "    plt.xlabel('Ano')\n",
    "    plt.ylabel('Número de Ocorrências')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    save_path_years = os.path.join(report_dir, 'records_per_year.png')\n",
    "    plt.savefig(save_path_years)\n",
    "    print(f\"Gráfico Registos por Ano salvo em: {save_path_years}\")\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"Dados de registos por ano não disponíveis para plotagem.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb017d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_df' in locals() and not sample_df.empty:\n",
    "    print(f\"Gerando mapa interativo com {len(sample_df)} pontos da amostra...\")\n",
    "    try:\n",
    "        fig = px.scatter_geo(\n",
    "            sample_df,\n",
    "            lat='decimalLatitude',\n",
    "            lon='decimalLongitude',\n",
    "            color='scientificName', # Colorir por espécie (pode ficar lento com muitas espécies)\n",
    "            hover_name='scientificName',\n",
    "            hover_data=['parsedDate'],\n",
    "            projection='natural earth',\n",
    "            title='Distribuição Geográfica das Ocorrências (Amostra)',\n",
    "            # Limitar o número de categorias de cores se houver muitas espécies\n",
    "            # color_discrete_sequence=px.colors.qualitative.Plotly[:10] # Exemplo\n",
    "        )\n",
    "        # Salvar como HTML interativo\n",
    "        map_path = os.path.join(report_dir, 'plankton_distribution_map.html')\n",
    "        fig.write_html(map_path)\n",
    "        print(f\"Mapa interativo salvo em: {map_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao gerar o mapa interativo: {e}\")\n",
    "else:\n",
    "    print(\"DataFrame de amostra não disponível para gerar mapa.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789346ee",
   "metadata": {},
   "source": [
    "## 4. Próximos Passos\n",
    "\n",
    "- Análise mais aprofundada das distribuições geográficas e temporais.\n",
    "- Limpeza de dados mais rigorosa (ex: validação de coordenadas, tratamento de outliers em `individualCount`).\n",
    "- Análise do ficheiro `verbatim.txt` se contiver informações adicionais úteis.\n",
    "- Download e EDA do dataset de Clorofila-a.\n",
    "- Integração dos dados de plâncton com os dados ambientais (SST, Clorofila-a).\n",
    "- Desenvolvimento do dashboard interativo.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
